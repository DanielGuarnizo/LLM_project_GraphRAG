{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Obtaining dependency information for langchain_community from https://files.pythonhosted.org/packages/d5/cb/582f22d74d69f4dbd41e98d361ee36922b79a245a9411383327bd4b63747/langchain_community-0.3.24-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting arxiv\n",
      "  Obtaining dependency information for arxiv from https://files.pythonhosted.org/packages/71/1e/e7f0393e836b5347605fc356c24d9f9ae9b26e0f7e52573b80e3d28335eb/arxiv-2.2.0-py3-none-any.whl.metadata\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/c4/92/4d681b5c066d417b98f22a0176358d9e606e183c6b61c337d61fb54accb4/tiktoken-0.9.0-cp39-cp39-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.9.0-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting langchainhub\n",
      "  Obtaining dependency information for langchainhub from https://files.pythonhosted.org/packages/35/63/40328157ddee807991f2f1992c2ad88f479b2472dc9e40d08ccf10700735/langchainhub-0.1.21-py3-none-any.whl.metadata\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting pymilvus\n",
      "  Obtaining dependency information for pymilvus from https://files.pythonhosted.org/packages/9d/3c/f28eca6a607009f6a3f955efef3e0d6bf4584b2d41431a0e062bbc1d0969/pymilvus-2.5.9-py3-none-any.whl.metadata\n",
      "  Downloading pymilvus-2.5.9-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/ed/5c/5c0be747261e1f8129b875fa3bfea736bc5fe17652f9d5e15ca118571b6f/langchain-0.3.25-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langgraph\n",
      "  Obtaining dependency information for langgraph from https://files.pythonhosted.org/packages/f2/2c/052922a444e32371f08eff4a8095e82cb74ed8a80823a9168079ce27211c/langgraph-0.4.5-py3-none-any.whl.metadata\n",
      "  Downloading langgraph-0.4.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tavily-python\n",
      "  Obtaining dependency information for tavily-python from https://files.pythonhosted.org/packages/47/85/57b6f6a53a5600cc11d6713b33bac16bf8d756bf30ddba71f7a64681755d/tavily_python-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tavily_python-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/45/2d/1151b371f28caae565ad384fdc38198f1165571870217aedda230b9d7497/sentence_transformers-4.1.0-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-milvus\n",
      "  Obtaining dependency information for langchain-milvus from https://files.pythonhosted.org/packages/93/ab/d321c9099ba395093b94d4cba6ba033f0d5e3e768ec096fe2f18dceab85d/langchain_milvus-0.1.10-py3-none-any.whl.metadata\n",
      "  Downloading langchain_milvus-0.1.10-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-ollama\n",
      "  Obtaining dependency information for langchain-ollama from https://files.pythonhosted.org/packages/84/6f/ab7a470522e27b95ed008eb9ef81b1ab55321f3f3aff21ca0109aae53cdf/langchain_ollama-0.3.3-py3-none-any.whl.metadata\n",
      "  Downloading langchain_ollama-0.3.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Obtaining dependency information for langchain-huggingface from https://files.pythonhosted.org/packages/0b/76/eb08f7b87f3377ced3800b2896841ccdcde3e246f46523946ecf092447e6/langchain_huggingface-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (4.12.2)\n",
      "Collecting beautifulsoup4\n",
      "  Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl.metadata\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langchain-experimental\n",
      "  Obtaining dependency information for langchain-experimental from https://files.pythonhosted.org/packages/b2/27/fe8caa4884611286b1f7d6c5cfd76e1fef188faaa946db4fde6daa1cd2cd/langchain_experimental-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting neo4j\n",
      "  Obtaining dependency information for neo4j from https://files.pythonhosted.org/packages/6a/57/94225fe5e9dabdc0ff60c88cbfcedf11277f4b34e7ab1373d3e62dbdd207/neo4j-5.28.1-py3-none-any.whl.metadata\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting json-repair\n",
      "  Obtaining dependency information for json-repair from https://files.pythonhosted.org/packages/c2/15/78602a6ccc4b5a9dd3fa031c8ae89ce1ccd89ae09d043b4d0838ddb553c0/json_repair-0.45.0-py3-none-any.whl.metadata\n",
      "  Downloading json_repair-0.45.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langchain-openai\n",
      "  Obtaining dependency information for langchain-openai from https://files.pythonhosted.org/packages/a9/60/886dc53c91031e26542f7ac1ea4062b7ebe542d22970996acaee59aa1cab/langchain_openai-0.3.17-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain_community)\n",
      "  Obtaining dependency information for langchain-core<1.0.0,>=0.3.59 from https://files.pythonhosted.org/packages/2d/bc/344f5b11fdfe0e27f7064d2e829921a791461dc32e5ed285fe6325518c26/langchain_core-0.3.60-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/dd/1c/3d2a893c020fcc18463794e0a687de58044d1c8a9892d23548ca7e71274a/sqlalchemy-2.0.41-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading sqlalchemy-2.0.41-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from langchain_community) (3.9.3)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n",
      "  Obtaining dependency information for tenacity!=8.4.0,<10,>=8.1.0 from https://files.pythonhosted.org/packages/e5/30/643397144bfbfec6f6ef821f36f33e57d35946c44a2352d3c9f0ae847619/tenacity-9.1.2-py3-none-any.whl.metadata\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl.metadata\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Obtaining dependency information for pydantic-settings<3.0.0,>=2.4.0 from https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl.metadata\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
      "  Obtaining dependency information for langsmith<0.4,>=0.1.125 from https://files.pythonhosted.org/packages/89/8e/e8a58e0abaae3f3ac4702e9ca35d1fc6159711556b64ffd0e247771a3f12/langsmith-0.3.42-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Obtaining dependency information for httpx-sse<1.0.0,>=0.4.0 from https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl.metadata\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy>=1.26.2 (from langchain_community)\n",
      "  Obtaining dependency information for numpy>=1.26.2 from https://files.pythonhosted.org/packages/43/c1/41c8f6df3162b0c6ffd4437d729115704bd43363de0090c7f913cfbc2d89/numpy-2.0.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting feedparser~=6.0.10 (from arxiv)\n",
      "  Obtaining dependency information for feedparser~=6.0.10 from https://files.pythonhosted.org/packages/7c/d4/8c31aad9cc18f451c49f7f9cfb5799dadffc88177f7917bc90a66459b1d7/feedparser-6.0.11-py3-none-any.whl.metadata\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting requests<3,>=2 (from langchain_community)\n",
      "  Obtaining dependency information for requests<3,>=2 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Obtaining dependency information for regex>=2022.1.18 from https://files.pythonhosted.org/packages/3c/8b/45c24ab7a51a1658441b961b86209c43e6bb9d39caf1e63f46ce6ea03bc7/regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<25,>=23.2 (from langchainhub)\n",
      "  Obtaining dependency information for packaging<25,>=23.2 from https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl.metadata\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Obtaining dependency information for types-requests<3.0.0.0,>=2.31.0.2 from https://files.pythonhosted.org/packages/fe/0f/68a997c73a129287785f418c1ebb6004f81e46b53b3caba88c0e03fcd04a/types_requests-2.32.0.20250515-py3-none-any.whl.metadata\n",
      "  Downloading types_requests-2.32.0.20250515-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting setuptools>69 (from pymilvus)\n",
      "  Obtaining dependency information for setuptools>69 from https://files.pythonhosted.org/packages/a1/18/0e835c3a557dc5faffc8f91092f62fc337c1dab1066715842e7a4b318ec4/setuptools-80.7.1-py3-none-any.whl.metadata\n",
      "  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<=1.67.1,>=1.49.1 (from pymilvus)\n",
      "  Obtaining dependency information for grpcio<=1.67.1,>=1.49.1 from https://files.pythonhosted.org/packages/91/d7/685b53b4dd7b5fffc0c48bc411065420136ab618d838f09ce41809233e2f/grpcio-1.67.1-cp39-cp39-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading grpcio-1.67.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from pymilvus) (3.20.3)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus)\n",
      "  Obtaining dependency information for python-dotenv<2.0.0,>=1.0.1 from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from pymilvus) (5.4.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from pymilvus) (1.5.3)\n",
      "Collecting milvus-lite>=2.4.0 (from pymilvus)\n",
      "  Obtaining dependency information for milvus-lite>=2.4.0 from https://files.pythonhosted.org/packages/64/3a/110e46db650ced604f97307e48e353726cfa6d26b1bf72acb81bbf07ecbd/milvus_lite-2.4.12-py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading milvus_lite-2.4.12-py3-none-macosx_10_9_x86_64.whl.metadata (10.0 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Obtaining dependency information for langchain-text-splitters<1.0.0,>=0.3.8 from https://files.pythonhosted.org/packages/8b/a3/3696ff2444658053c01b6b7443e761f28bb71217d82bb89137a978c5f66f/langchain_text_splitters-0.3.8-py3-none-any.whl.metadata\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Obtaining dependency information for pydantic<3.0.0,>=2.7.4 from https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.26 (from langgraph)\n",
      "  Obtaining dependency information for langgraph-checkpoint<3.0.0,>=2.0.26 from https://files.pythonhosted.org/packages/38/48/d7cec540a3011b3207470bb07294a399e3b94b2e8a602e38cb007ce5bc10/langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata\n",
      "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt>=0.1.8 (from langgraph)\n",
      "  Obtaining dependency information for langgraph-prebuilt>=0.1.8 from https://files.pythonhosted.org/packages/36/72/9e092665502f8f52f2708065ed14fbbba3f95d1a1b65d62049b0c5fcdf00/langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata\n",
      "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
      "  Obtaining dependency information for langgraph-sdk>=0.1.42 from https://files.pythonhosted.org/packages/b0/e6/8e82a0373e233392d83ae37f473c9799c536b307322f0caf49a59bce9522/langgraph_sdk-0.1.69-py3-none-any.whl.metadata\n",
      "  Downloading langgraph_sdk-0.1.69-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
      "  Obtaining dependency information for xxhash<4.0.0,>=3.5.0 from https://files.pythonhosted.org/packages/d4/f6/531dd6858adf8877675270b9d6989b6dacfd1c2d7135b17584fc29866df3/xxhash-3.5.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting httpx (from tavily-python)\n",
      "  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.41.0 from https://files.pythonhosted.org/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.7.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.20.0 from https://files.pythonhosted.org/packages/33/c7/852d4473788cfd7d79b73951244b87a6d75fdac296c90aeb5e85dbb2fb5e/huggingface_hub-0.31.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (4.5.0)\n",
      "Collecting ollama<1.0.0,>=0.4.8 (from langchain-ollama)\n",
      "  Obtaining dependency information for ollama<1.0.0,>=0.4.8 from https://files.pythonhosted.org/packages/33/3f/164de150e983b3a16e8bf3d4355625e51a357e7b3b1deebe9cc1f7cb9af8/ollama-0.4.8-py3-none-any.whl.metadata\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Obtaining dependency information for tokenizers>=0.19.1 from https://files.pythonhosted.org/packages/a5/1f/328aee25f9115bf04262e8b4e5a2050b7b7cf44b59c74e982db7270c7f30/tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pytz in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from neo4j) (2024.1)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Obtaining dependency information for openai<2.0.0,>=1.68.2 from https://files.pythonhosted.org/packages/81/d2/e3992bb7c6641b765c1008e3c96e076e0b50381be2cce344e6ff177bad80/openai-1.79.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.79.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/34/75/51952c7b2d3873b44a0028b1bd26a25078c18f92f256608e8d1dc61b39fd/marshmallow-3.26.1-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/44/4b/e0cfc1a6f17e990f3e64b7d941ddc4acdc7b19d6edd51abf495f32b1a9e4/fsspec-2025.3.2-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for typing_extensions>=4.5.0 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.26->langgraph)\n",
      "  Obtaining dependency information for ormsgpack<2.0.0,>=1.8.0 from https://files.pythonhosted.org/packages/85/02/ac4a2263c9aad0d455f240e1bdd41b443e5452257cf13bc188177b0dfd1f/ormsgpack-1.9.1-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata\n",
      "  Downloading ormsgpack-1.9.1-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.10.1 (from langgraph-sdk>=0.1.42->langgraph)\n",
      "  Obtaining dependency information for orjson>=3.10.1 from https://files.pythonhosted.org/packages/df/db/69488acaa2316788b7e171f024912c6fe8193aa2e24e9cfc7bc41c3669ba/orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata\n",
      "  Downloading orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from httpx->tavily-python) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from httpx->tavily-python) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx->tavily-python)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from httpx->tavily-python) (3.4)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx->tavily-python)\n",
      "  Obtaining dependency information for h11>=0.16 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Obtaining dependency information for zstandard<0.24.0,>=0.23.0 from https://files.pythonhosted.org/packages/fb/96/4fcafeb7e013a2386d22f974b5b97a0b9a65004ed58c87ae001599bfbd48/zstandard-0.23.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading zstandard-0.23.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/98/fd/aced428e2bd3c6c1132f67c5a708f9e7fd161d0ca8f8c5862b17b93cdf0a/jiter-0.10.0-cp39-cp39-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading jiter-0.10.0-cp39-cp39-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Obtaining dependency information for pydantic-core==2.33.2 from https://files.pythonhosted.org/packages/53/ea/bbe9095cdd771987d13c82d104a9c8559ae9aec1e29f139e286fd2e9256e/pydantic_core-2.33.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain_community) (2.1.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Downloading greenlet-3.2.2.tar.gz (185 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sympy in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.3 from https://files.pythonhosted.org/packages/18/ae/88f6c49dbd0cc4da0e08610019a3c78a7d390879a919411a410a1876d03a/safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/7f/29/c2ea58c9731b9ecb30b6738113a95d147e83922986b34c685b8f6eefde21/scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from anyio->httpx->tavily-python) (1.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielguarnizo/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading tiktoken-0.9.0-cp39-cp39-macosx_10_12_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Downloading pymilvus-2.5.9-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading langgraph-0.4.5-py3-none-any.whl (155 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tavily_python-0.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_milvus-0.1.10-py3-none-any.whl (29 kB)\n",
      "Downloading langchain_ollama-0.3.3-py3-none-any.whl (21 kB)\n",
      "Downloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading json_repair-0.45.0-py3-none-any.whl (22 kB)\n",
      "Downloading langchain_openai-0.3.17-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp39-cp39-macosx_10_9_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.3/489.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
      "Downloading langgraph_sdk-0.1.69-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading milvus_lite-2.4.12-py3-none-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp39-cp39-macosx_10_9_x86_64.whl (21.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/21.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp39-cp39-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.7/287.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
      "Downloading sqlalchemy-2.0.41-cp39-cp39-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.0.20250515-py3-none-any.whl (20 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-macosx_10_9_x86_64.whl (31 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl (39.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp39-cp39-macosx_10_12_x86_64.whl (317 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.7/317.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.3/249.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ormsgpack-1.9.1-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (382 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.8/382.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-macosx_10_12_x86_64.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.9/436.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp39-cp39-macosx_10_9_x86_64.whl (788 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.7/788.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: greenlet, sgmllib3k\n",
      "  Building wheel for greenlet (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for greenlet: filename=greenlet-3.2.2-cp39-cp39-macosx_10_9_x86_64.whl size=217516 sha256=7b139b703befaec760562bb64e63a9173ddc36e8a6c879b3adcbb56e3e0bbdcf\n",
      "  Stored in directory: /Users/danielguarnizo/Library/Caches/pip/wheels/4f/da/fb/17774d01d5b9618c3159bcd7cc873e8ceafdc089cf4570b031\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=8019aa7b173517b9a01707f3480b575d7f1b1a58166d4dce274d14431c630ca9\n",
      "  Stored in directory: /Users/danielguarnizo/Library/Caches/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built greenlet sgmllib3k\n",
      "Installing collected packages: sgmllib3k, zstandard, xxhash, typing_extensions, types-requests, tenacity, setuptools, safetensors, requests, regex, python-dotenv, packaging, ormsgpack, orjson, numpy, neo4j, milvus-lite, json-repair, jiter, httpx-sse, h11, grpcio, greenlet, fsspec, feedparser, distro, annotated-types, typing-inspection, typing-inspect, tiktoken, SQLAlchemy, scipy, pydantic-core, marshmallow, langchainhub, huggingface-hub, httpcore, beautifulsoup4, arxiv, tokenizers, pymilvus, pydantic, httpx, dataclasses-json, transformers, tavily-python, pydantic-settings, openai, ollama, langsmith, langgraph-sdk, sentence-transformers, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-ollama, langchain-milvus, langchain-huggingface, langgraph-prebuilt, langchain, langgraph, langchain_community, langchain-experimental\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.0.0\n",
      "    Uninstalling setuptools-68.0.0:\n",
      "      Successfully uninstalled setuptools-68.0.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "python-lsp-black 2.0.0 requires black>=23.11.0, but you have black 22.3.0 which is incompatible.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SQLAlchemy-2.0.41 annotated-types-0.7.0 arxiv-2.2.0 beautifulsoup4-4.13.4 dataclasses-json-0.6.7 distro-1.9.0 feedparser-6.0.11 fsspec-2025.3.2 greenlet-3.2.2 grpcio-1.57.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.31.4 jiter-0.10.0 json-repair-0.45.0 langchain-0.3.25 langchain-core-0.3.60 langchain-experimental-0.3.4 langchain-huggingface-0.2.0 langchain-milvus-0.1.10 langchain-ollama-0.3.3 langchain-openai-0.3.17 langchain-text-splitters-0.3.8 langchain_community-0.3.24 langchainhub-0.1.21 langgraph-0.4.5 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.69 langsmith-0.3.42 marshmallow-3.26.1 milvus-lite-2.4.12 neo4j-5.28.1 numpy-2.0.2 ollama-0.4.8 openai-1.79.0 orjson-3.10.18 ormsgpack-1.9.1 packaging-24.2 pydantic-2.11.4 pydantic-core-2.33.2 pydantic-settings-2.9.1 pymilvus-2.5.9 python-dotenv-1.1.0 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scipy-1.13.1 sentence-transformers-4.1.0 setuptools-80.7.1 sgmllib3k-1.0.0 tavily-python-0.7.2 tenacity-9.1.2 tiktoken-0.9.0 tokenizers-0.21.1 transformers-4.51.3 types-requests-2.32.0.20250515 typing-inspect-0.9.0 typing-inspection-0.4.0 typing_extensions-4.13.2 xxhash-3.5.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph GraphRAG agent with Llama 3.x and GPT4o\n",
    "\n",
    "\n",
    "Let's build an Advanced RAG with a GraphRAG agent that will run a combination of Llama 3.1 and GPT4o, for Llama 3.1 we will use Ollama. The idea is that we use GPT4o for advanced tasks, like generating the Neo4j query and Llama3.1 for the rest. \n",
    "\n",
    "## Ideas\n",
    "\n",
    "We'll combine ideas from three RAG papers into a RAG agent:\n",
    "\n",
    "- **Routing:**  Adaptive RAG ([paper](https://arxiv.org/abs/2403.14403)). Route questions to different retrieval approaches\n",
    "- **Fallback:** Corrective RAG ([paper](https://arxiv.org/pdf/2401.15884.pdf)). Fallback to web search if docs are not relevant to query\n",
    "- **Self-correction:** Self-RAG ([paper](https://arxiv.org/abs/2310.11511)). Fix answers w/ hallucinations or don’t address question\n",
    "\n",
    "![langgraph_adaptive_rag.png](imgs/RAG_Agent_langGraph.png)\n",
    "\n",
    "Note that this will incorperate [a few general ideas for agents](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/):\n",
    "\n",
    "- **Reflection**: The self-correction mechanism is a form of reflection, where the LangGraph agent reflects on its retrieval and generations\n",
    "- **Planning**: The control flow laid out in the graph is a form of planning \n",
    "- **Tool use**: Specific nodes in the control flow (e.g., web search) will use tools\n",
    "\n",
    "## Local models\n",
    "\n",
    "### LLM\n",
    "\n",
    "Use [Ollama](https://ollama.ai/) and [llama3](https://ollama.ai/library/llama3):\n",
    "\n",
    "```\n",
    "ollama pull llama3.1\n",
    "```\n",
    "\n",
    "### Env Variables\n",
    "Variables needed in an .env file or loaded as variables at start:\n",
    "\n",
    "Required:\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "TAVILY_API_KEY=tvly-...\n",
    "NEO4J_URI=...\n",
    "NEO4J_USERNAME=...\n",
    "NEO4J_PASSWORD=...\n",
    "```\n",
    "\n",
    "### Search\n",
    "\n",
    "Uses [Tavily](https://tavily.com/#api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the project to clear reading in langchain smith \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Graph RAG: using Neo4j, Milvus and Ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">Extract Data and process it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsome useful thing to undestand what is a Result object from arxiv\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "some useful thing to undestand what is a Result object from arxiv\n",
    "\"\"\"\n",
    "# search_query = \"agent OR 'large language model' OR 'prompt engineering'\"\n",
    "# max_results = 20\n",
    "\n",
    "# client = arxiv.Client()\n",
    "# search = arxiv.Search(\n",
    "#     query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    "# )\n",
    "# results = client.results(search)\n",
    "# result = None\n",
    "# for result in results:\n",
    "#     result = result\n",
    "#     print(result)\n",
    "#     break\n",
    "# type(result)\n",
    "\n",
    "# print(result.summary)\n",
    "# result.download_source()\n",
    "# result.download_pdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_split_documents(\n",
    "    search_query: str,\n",
    "    max_results: int = 20\n",
    "):\n",
    "    # Fetch papers from arXiv\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=search_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = client.results(search)\n",
    "\n",
    "    # Collect document summaries\n",
    "    docs = []\n",
    "    for result in results:\n",
    "        docs.append({\n",
    "            \"title\": result.title,\n",
    "            \"summary\": result.summary,\n",
    "            \"url\": result.entry_id\n",
    "        })\n",
    "\n",
    "    # Split summaries into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=50\n",
    "    )\n",
    "    doc_splits = text_splitter.create_documents(\n",
    "        [doc[\"summary\"] for doc in docs], metadatas=docs\n",
    "    )\n",
    "\n",
    "    print(f\"Number of papers: {len(docs)}\")\n",
    "    print(f\"Number of chunks: {len(doc_splits)}\")\n",
    "    return doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_retriever_from_splits_if_needed(\n",
    "    doc_splits,\n",
    "    milvus_db_path: str = \"./milvus_ingest_v2.db\",\n",
    "    collection_name: str = \"rag_milvus\"\n",
    "):\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    if os.path.exists(milvus_db_path):\n",
    "        print(\"Milvus DB exists. Connecting to existing database...\")\n",
    "        vectorstore = Milvus(\n",
    "            embedding_function=embedding_model,\n",
    "            collection_name=collection_name,\n",
    "            connection_args={\"uri\": milvus_db_path},\n",
    "        )\n",
    "    else:\n",
    "        print(\"Milvus DB not found. Ingesting provided documents...\")\n",
    "        vectorstore = Milvus.from_documents(\n",
    "            documents=doc_splits,\n",
    "            collection_name=collection_name,\n",
    "            embedding=embedding_model,\n",
    "            connection_args={\"uri\": milvus_db_path},\n",
    "        )\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more related arguments\n",
    "# query = \"agent OR 'large language model' OR 'prompt engineering'\"\n",
    "# doc_splits = fetch_and_split_documents(search_query=query)\n",
    "# retriever = setup_retriever_from_splits_if_needed(doc_splits, milvus_db_path=\"./milvus_ingest_v2.db\") # change name of db_path if the search query is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Number of papers: 30\n",
      "🧩 Number of chunks: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/lqj95by96lnccb74l6fsytf00000gn/T/ipykernel_42221/213781723.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🆕 Milvus DB not found. Ingesting provided documents...\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"('large language model' OR 'prompt engineering') \"\n",
    "    \"OR ('multi-agent' OR 'agent-based') \"\n",
    "    \"OR ('theory of mind' OR 'cognitive modeling' OR 'belief modeling')\"\n",
    ")\n",
    "\n",
    "doc_splits = fetch_and_split_documents(search_query=query, max_results=30)\n",
    "\n",
    "# Save to vector DB for vector-based RAG\n",
    "retriever = setup_retriever_from_splits_if_needed(\n",
    "    doc_splits,\n",
    "    milvus_db_path=\"./milvus_graph_vs_vector_rag.db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> Process data to construct Knowlege graph using GPT-4o and store it in Neo4j database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = Neo4jGraph()\n",
    "\n",
    "\n",
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provious set up \n",
    "\n",
    "# graph_transformer = LLMGraphTransformer(\n",
    "#     llm=graph_llm,\n",
    "#     allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "#     node_properties=[\"title\", \"summary\", \"url\"],\n",
    "#     allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    "# )\n",
    "\n",
    "# Knowledge Graph setup\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\", \"Method\", \"Dataset\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\", \"published\"],\n",
    "    allowed_relationships=[\n",
    "        \"AUTHORED\",\n",
    "        \"DISCUSSES\",\n",
    "        \"RELATED_TO\",\n",
    "        \"USES_METHOD\",\n",
    "        \"USES_DATASET\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# run only when need get graph based on doc_splits\n",
    "# graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if os.path.exists(\"./graph_documents/graph_documents.pkl\"):\n",
    "    # Load (on next run)\n",
    "    with open(\"./graph_documents/graph_documents.pkl\", \"rb\") as f:\n",
    "        graph_documents = pickle.load(f)\n",
    "else:\n",
    "    # run only when need get graph based on doc_splits\n",
    "    graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "    # Save\n",
    "    with open(\"./graph_documents/graph_documents.pkl\", \"wb\") as f:\n",
    "        pickle.dump(graph_documents, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! RUN IT ONLY WHEN YOU ARE SURE TO STORE IN THE NEO4J DATABASE\n",
    "kg.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n': {'id': 'Signal Control Problem'}}, {'n': {'id': 'Various Existing Methods'}}, {'n': {'id': 'Multiple Agents'}}, {'n': {'id': 'Fine-Tuned Large Language Model Approach'}}, {'n': {'id': '2 Datasets'}}, {'n': {'id': 'Human Interactions'}}, {'n': {'id': 'Theory Of Mind'}}, {'n': {'id': 'Large Language Models'}}, {'n': {'id': 'Chain-Of-Thought'}}, {'n': {'id': 'Simulation Theory'}}, {'n': {'id': 'Simtom'}}, {'n': {'id': 'Tom Benchmarks'}}, {'n': {'id': 'Beliefnest'}}, {'n': {'id': 'Minecraft'}}, {'n': {'id': 'Large Language Models'}}, {'n': {'id': 'Chain Of Thought'}}, {'n': {'id': 'Timetom'}}, {'n': {'id': 'Temporal Belief State Chain'}}, {'n': {'id': 'Tool-Belief Solver'}}, {'n': {'id': 'Emotional Support Systems'}}, {'n': {'id': 'Mind-To-Mind (Mind2)'}}, {'n': {'id': 'Cognitive Models'}}, {'n': {'id': 'Theory-Of-Mind'}}, {'n': {'id': 'Physiological Expected Utility'}}, {'n': {'id': 'Cognitive Rationality'}}, {'n': {'id': \"How Do People Understand And Evaluate Claims About Others' Beliefs, Even Though These Beliefs Cannot Be Directly Observed?\", 'title': \"How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed?\"}}, {'n': {'id': 'Language-Augmented Bayesian Theory-Of-Mind (Labtom)'}}, {'n': {'id': 'Multimodal Llms (Gpt-4O, Gemini Pro)'}}, {'n': {'id': 'Minddial'}}, {'n': {'id': 'Mind Module'}}, {'n': {'id': 'Prompting And Fine-Tuning-Based Models'}}, {'n': {'id': 'The Use Of Llms In Natural Language Reasoning'}}, {'n': {'id': 'Theory-Of-Mind (Tom) Reasoning'}}, {'n': {'id': 'Role-Playing Prompting'}}, {'n': {'summary': 'The paper proposes adequacy conditions for a representation in an LLM to count as belief-like, establishing four criteria: accuracy, coherence, uniformity, and use.', 'id': 'Belief Representation In Llms'}}, {'n': {'id': 'Accuracy'}}, {'n': {'id': 'Coherence'}}, {'n': {'id': 'Uniformity'}}, {'n': {'id': 'Use'}}, {'n': {'id': 'Belief Measurement In Decision Theory'}}, {'n': {'id': 'Formal Epistemology'}}, {'n': {'id': 'Philosophy'}}, {'n': {'id': 'Machine Learning'}}, {'n': {'id': 'Ai'}}, {'n': {'id': 'Nlp Systems'}}, {'n': {'id': 'Coke'}}, {'n': {'id': 'Colm'}}, {'n': {'id': 'Cognitive Chains'}}, {'n': {'id': 'Cognitive Reasoning'}}, {'n': {'id': 'This Paper', 'title': 'COKE: the first cognitive knowledge graph for machine theory of mind'}}, {'n': {'summary': 'The paper explores the cognitive capacities of Large Language Models (LLMs), particularly their ability to reason about intentions and beliefs, known as Theory of Mind (ToM). It tests 11 base- and instruction-tuned LLMs on capabilities relevant to ToM, using newly rewritten versions of standardized tests, and benchmarks LLM performance against children aged 7-10.', 'id': 'Cognitive Capacities Of Llms', 'title': 'Cognitive Capacities of LLMs'}}, {'n': {'id': 'Instruction-Tuned Llms'}}, {'n': {'id': 'Base-Llms'}}, {'n': {'id': 'Gpt Family'}}, {'n': {'id': 'Children Aged 7-10'}}, {'n': {'id': 'Psychology'}}, {'n': {'id': 'Robotics'}}, {'n': {'id': 'Developmental Synergy'}}, {'n': {'id': 'Feed-Forward Deep Learning Model'}}, {'n': {'id': 'Human Social Cognitive Development'}}, {'n': {'id': 'Adaptive Social Robots'}}, {'n': {'id': 'Theory Of Mind (Tom)'}}, {'n': {'id': 'Artificial Intelligence (Ai)'}}, {'n': {'id': 'Model'}}, {'n': {'id': \"Guilford'S Structure Of Intellect (Soi) Model\"}}, {'n': {'id': 'Cognitive Prompt Engineering'}}, {'n': {'id': 'Novel Cognitive Prompting Approach'}}, {'n': {'summary': 'This position paper presents a novel cognitive prompting approach for enforcing SOI-inspired reasoning for improving clarity, coherence, and adaptability in model responses.', 'id': 'Position Paper'}}, {'n': {'id': 'Theory-Of-Mind Tasks'}}, {'n': {'id': 'Gpt-4'}}, {'n': {'id': 'Gpt-3.5 Variants'}}, {'n': {'id': 'Davinci-2'}}, {'n': {'id': 'Davinci-3'}}, {'n': {'id': 'Gpt-3.5-Turbo'}}, {'n': {'id': 'Reinforcement Learning From Human Feedback'}}, {'n': {'id': 'In-Context Learning'}}, {'n': {'id': 'Two-Shot Chain Of Thought Reasoning'}}, {'n': {'id': 'Step-By-Step Thinking Instructions'}}, {'n': {'id': 'Trust-Aware Robot Policy'}}, {'n': {'id': 'Dynamic Trust-Aware Reward Function'}}, {'n': {'id': 'Human-Robot Trust'}}, {'n': {'id': 'Tom-Based Robot Policy'}}, {'n': {'id': 'Mtomnet'}}, {'n': {'id': 'Belief Prediction Dataset'}}, {'n': {'id': 'Belief Dynamics Prediction Dataset'}}, {'n': {'id': 'Large-Scale Neural Language Models'}}, {'n': {'id': 'Symbolictom'}}, {'n': {'id': 'Tomi Benchmark'}}, {'n': {'id': 'Le Et Al., 2019'}}, {'n': {'id': 'Off-The-Shelf Neural Networks'}}, {'n': {'summary': 'The paper studies a simple model of causal cognition, showing that under mild assumptions, self-confirming causal models exist, which perpetrate self-deception and challenge objectivity.', 'id': 'In The Present Paper, We Study A Simple Model Of Causal Cognition, Viewed As A Quest For Causal Models'}}, {'n': {'id': 'Causal Cognition'}}, {'n': {'id': 'Self-Confirming Causal Models'}}, {'n': {'summary': \"BDIQA is the first benchmark to explore cognitive reasoning capabilities of VideoQA models in the context of Theory of Mind (ToM), inspired by children's cognitive development, offering tasks at two difficulty levels assessing Belief, Desire, and Intention (BDI) reasoning.\", 'id': 'Bdiqa', 'title': 'BDIQA: Benchmark for Cognitive Reasoning in VideoQA'}}, {'n': {'id': 'Videoqa'}}, {'n': {'id': 'Dorsal Medial Prefrontal Cortex Neurons'}}, {'n': {'id': 'Hidden Embeddings'}}, {'n': {'id': 'Tom Tasks'}}, {'n': {'id': 'Integration Of Ai Subdisciplines'}}, {'n': {'id': 'Cognitive Architectures'}}, {'n': {'id': 'Modular Approach'}}, {'n': {'id': 'Agency Approach'}}, {'n': {'id': 'Neuro-Symbolic Approach'}}, {'n': {'id': 'Chain-Of-Thought Prompting'}}, {'n': {'id': 'Augmented Llms'}}, {'n': {'id': 'Common Model Of Cognition'}}, {'n': {'id': 'Simulation Theory Of Cognition'}}, {'n': {'id': 'Society Of Mind Theory'}}, {'n': {'id': 'Lida Cognitive Architecture'}}, {'n': {'id': 'Clarion Cognitive Architecture'}}, {'n': {'id': 'Humans With An Average Level Of Social Cognition'}}, {'n': {'id': 'Nonverbal Communication Signals'}}, {'n': {'id': 'Social Cognitive Ability'}}, {'n': {'id': 'Human-Robot Interaction And Collaboration'}}, {'n': {'id': 'Object-Context Relations'}}, {'n': {'id': 'Multimodal Video Dataset'}}, {'n': {'id': 'Artificial Intelligence (Ai) Systems'}}, {'n': {'id': 'Deep Learning Models'}}, {'n': {'id': 'Hi-Tom'}}, {'n': {'id': 'Higher-Order Theory Of Mind'}}, {'n': {'id': 'Higher Order Theory Of Mind Benchmark'}}, {'n': {'id': 'Social Reasoning'}}, {'n': {'id': 'Machine Learning Approaches To Tom'}}, {'n': {'id': 'Tommy'}}, {'n': {'id': 'New Suite Of Experiments'}}, {'n': {'id': 'Virtual World Cognitive Science'}}, {'n': {'id': 'Mental And Linguistic Representation'}}, {'n': {'id': 'Cognitive Science'}}, {'n': {'id': 'Prompt Engineering'}}, {'n': {'id': 'Natural Language Processing'}}, {'n': {'id': 'Vision-Language Modeling'}}, {'n': {'id': 'Multimodal-To-Text Generation Models'}}, {'n': {'id': 'Image-Text Matching Models'}}, {'n': {'id': 'Text-To-Image Generation Models'}}, {'n': {'id': 'Flamingo'}}, {'n': {'id': 'Clip'}}, {'n': {'id': 'Stable Diffusion'}}, {'n': {'id': 'Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', 'title': 'Designing effective prompts is essential to guiding large language models (LLMs) toward desired responses'}}, {'n': {'id': 'Automated Prompt Engineering'}}, {'n': {'id': 'Optimal Learning Framework'}}, {'n': {'id': 'Feature-Based Method'}}, {'n': {'id': 'Bayesian Regression'}}, {'n': {'id': 'Knowledge-Gradient (Kg) Policy'}}, {'n': {'id': 'Whole Hog Thesis'}}, {'n': {'id': 'Sophisticated Large Language Models'}}, {'n': {'id': 'Chatgpt'}}, {'n': {'id': 'Holistic Network Assumptions'}}, {'n': {'id': 'Interaction With Large Language Models', 'title': 'Interaction with Large Language Models'}}, {'n': {'id': 'Prompting'}}, {'n': {'id': 'Prompt Design'}}]\n"
     ]
    }
   ],
   "source": [
    "# Run Cypher\n",
    "results = kg.query(\"MATCH (n) RETURN n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph documents: 30\n",
      "Nodes from 1st graph doc:[Node(id='Signal Control Problem', type='Topic', properties={}), Node(id='Various Existing Methods', type='Method', properties={}), Node(id='Multiple Agents', type='Method', properties={}), Node(id='Fine-Tuned Large Language Model Approach', type='Method', properties={}), Node(id='2 Datasets', type='Dataset', properties={})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='Various Existing Methods', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='Multiple Agents', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='Fine-Tuned Large Language Model Approach', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='2 Datasets', type='Dataset', properties={}), type='USES_DATASET', properties={})]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "  Nodes: [Node(id='Signal Control Problem', type='Topic', properties={}), Node(id='Various Existing Methods', type='Method', properties={}), Node(id='Multiple Agents', type='Method', properties={}), Node(id='Fine-Tuned Large Language Model Approach', type='Method', properties={}), Node(id='2 Datasets', type='Dataset', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='Various Existing Methods', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='Multiple Agents', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='Fine-Tuned Large Language Model Approach', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Signal Control Problem', type='Topic', properties={}), target=Node(id='2 Datasets', type='Dataset', properties={}), type='USES_DATASET', properties={})]\n",
      "---\n",
      "Document 1:\n",
      "  Nodes: [Node(id='Human Interactions', type='Topic', properties={}), Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Large Language Models', type='Topic', properties={}), Node(id='Chain-Of-Thought', type='Method', properties={}), Node(id='Simulation Theory', type='Topic', properties={}), Node(id='Simtom', type='Method', properties={}), Node(id='Tom Benchmarks', type='Dataset', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Human Interactions', type='Topic', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Theory Of Mind', type='Topic', properties={}), target=Node(id='Large Language Models', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Chain-Of-Thought', type='Method', properties={}), target=Node(id='Large Language Models', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Simtom', type='Method', properties={}), target=Node(id='Simulation Theory', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Simtom', type='Method', properties={}), target=Node(id='Tom Benchmarks', type='Dataset', properties={}), type='USES_DATASET', properties={})]\n",
      "---\n",
      "Document 2:\n",
      "  Nodes: [Node(id='Beliefnest', type='Method', properties={}), Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Minecraft', type='Dataset', properties={}), Node(id='Large Language Models', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Beliefnest', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Beliefnest', type='Method', properties={}), target=Node(id='Minecraft', type='Dataset', properties={}), type='USES_DATASET', properties={}), Relationship(source=Node(id='Beliefnest', type='Method', properties={}), target=Node(id='Large Language Models', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 3:\n",
      "  Nodes: [Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Large Language Models', type='Topic', properties={}), Node(id='Chain Of Thought', type='Method', properties={}), Node(id='Timetom', type='Method', properties={}), Node(id='Temporal Belief State Chain', type='Method', properties={}), Node(id='Tool-Belief Solver', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Theory Of Mind', type='Topic', properties={}), target=Node(id='Large Language Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Timetom', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Timetom', type='Method', properties={}), target=Node(id='Large Language Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Timetom', type='Method', properties={}), target=Node(id='Temporal Belief State Chain', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Timetom', type='Method', properties={}), target=Node(id='Tool-Belief Solver', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Temporal Belief State Chain', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Tool-Belief Solver', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 4:\n",
      "  Nodes: [Node(id='Emotional Support Systems', type='Topic', properties={}), Node(id='Mind-To-Mind (Mind2)', type='Method', properties={}), Node(id='Cognitive Models', type='Method', properties={}), Node(id='Theory-Of-Mind', type='Method', properties={}), Node(id='Physiological Expected Utility', type='Method', properties={}), Node(id='Cognitive Rationality', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Mind-To-Mind (Mind2)', type='Method', properties={}), target=Node(id='Emotional Support Systems', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Mind-To-Mind (Mind2)', type='Method', properties={}), target=Node(id='Cognitive Models', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Mind-To-Mind (Mind2)', type='Method', properties={}), target=Node(id='Theory-Of-Mind', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Mind-To-Mind (Mind2)', type='Method', properties={}), target=Node(id='Physiological Expected Utility', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Mind-To-Mind (Mind2)', type='Method', properties={}), target=Node(id='Cognitive Rationality', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 5:\n",
      "  Nodes: [Node(id=\"How Do People Understand And Evaluate Claims About Others' Beliefs, Even Though These Beliefs Cannot Be Directly Observed?\", type='Paper', properties={'title': \"How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed?\"}), Node(id='Language-Augmented Bayesian Theory-Of-Mind (Labtom)', type='Method', properties={}), Node(id='Multimodal Llms (Gpt-4O, Gemini Pro)', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id=\"How Do People Understand And Evaluate Claims About Others' Beliefs, Even Though These Beliefs Cannot Be Directly Observed?\", type='Paper', properties={}), target=Node(id='Language-Augmented Bayesian Theory-Of-Mind (Labtom)', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id=\"How Do People Understand And Evaluate Claims About Others' Beliefs, Even Though These Beliefs Cannot Be Directly Observed?\", type='Paper', properties={}), target=Node(id='Multimodal Llms (Gpt-4O, Gemini Pro)', type='Method', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 6:\n",
      "  Nodes: [Node(id='Minddial', type='Method', properties={}), Node(id='Mind Module', type='Method', properties={}), Node(id='Prompting And Fine-Tuning-Based Models', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Minddial', type='Method', properties={}), target=Node(id='Mind Module', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Minddial', type='Method', properties={}), target=Node(id='Prompting And Fine-Tuning-Based Models', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 7:\n",
      "  Nodes: [Node(id='The Use Of Llms In Natural Language Reasoning', type='Paper', properties={}), Node(id='Theory-Of-Mind (Tom) Reasoning', type='Topic', properties={}), Node(id='Role-Playing Prompting', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='The Use Of Llms In Natural Language Reasoning', type='Paper', properties={}), target=Node(id='Theory-Of-Mind (Tom) Reasoning', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='The Use Of Llms In Natural Language Reasoning', type='Paper', properties={}), target=Node(id='Role-Playing Prompting', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 8:\n",
      "  Nodes: [Node(id='Belief Representation In Llms', type='Paper', properties={'summary': 'The paper proposes adequacy conditions for a representation in an LLM to count as belief-like, establishing four criteria: accuracy, coherence, uniformity, and use.'}), Node(id='Accuracy', type='Topic', properties={}), Node(id='Coherence', type='Topic', properties={}), Node(id='Uniformity', type='Topic', properties={}), Node(id='Use', type='Topic', properties={}), Node(id='Belief Measurement In Decision Theory', type='Topic', properties={}), Node(id='Formal Epistemology', type='Topic', properties={}), Node(id='Philosophy', type='Topic', properties={}), Node(id='Machine Learning', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Accuracy', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Coherence', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Uniformity', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Use', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Belief Measurement In Decision Theory', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Formal Epistemology', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Philosophy', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Belief Representation In Llms', type='Paper', properties={}), target=Node(id='Machine Learning', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 9:\n",
      "  Nodes: [Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Ai', type='Topic', properties={}), Node(id='Nlp Systems', type='Topic', properties={}), Node(id='Coke', type='Method', properties={}), Node(id='Colm', type='Method', properties={}), Node(id='Cognitive Chains', type='Dataset', properties={}), Node(id='Cognitive Reasoning', type='Topic', properties={}), Node(id='This Paper', type='Paper', properties={'title': 'COKE: the first cognitive knowledge graph for machine theory of mind'})]\n",
      "  Relationships: [Relationship(source=Node(id='Theory Of Mind', type='Topic', properties={}), target=Node(id='Ai', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Theory Of Mind', type='Topic', properties={}), target=Node(id='Nlp Systems', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Coke', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Coke', type='Method', properties={}), target=Node(id='Cognitive Chains', type='Dataset', properties={}), type='USES_DATASET', properties={}), Relationship(source=Node(id='Colm', type='Method', properties={}), target=Node(id='Cognitive Reasoning', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Coke', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Colm', type='Method', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 10:\n",
      "  Nodes: [Node(id='Cognitive Capacities Of Llms', type='Paper', properties={'title': 'Cognitive Capacities of LLMs', 'summary': 'The paper explores the cognitive capacities of Large Language Models (LLMs), particularly their ability to reason about intentions and beliefs, known as Theory of Mind (ToM). It tests 11 base- and instruction-tuned LLMs on capabilities relevant to ToM, using newly rewritten versions of standardized tests, and benchmarks LLM performance against children aged 7-10.'}), Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Instruction-Tuned Llms', type='Method', properties={}), Node(id='Base-Llms', type='Method', properties={}), Node(id='Gpt Family', type='Method', properties={}), Node(id='Children Aged 7-10', type='Dataset', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Cognitive Capacities Of Llms', type='Paper', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Cognitive Capacities Of Llms', type='Paper', properties={}), target=Node(id='Instruction-Tuned Llms', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Cognitive Capacities Of Llms', type='Paper', properties={}), target=Node(id='Base-Llms', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Cognitive Capacities Of Llms', type='Paper', properties={}), target=Node(id='Gpt Family', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Cognitive Capacities Of Llms', type='Paper', properties={}), target=Node(id='Children Aged 7-10', type='Dataset', properties={}), type='USES_DATASET', properties={})]\n",
      "---\n",
      "Document 11:\n",
      "  Nodes: [Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Psychology', type='Topic', properties={}), Node(id='Machine Learning', type='Topic', properties={}), Node(id='Robotics', type='Topic', properties={}), Node(id='Developmental Synergy', type='Topic', properties={}), Node(id='Feed-Forward Deep Learning Model', type='Method', properties={}), Node(id='Human Social Cognitive Development', type='Topic', properties={}), Node(id='Adaptive Social Robots', type='Topic', properties={}), Node(id='This Paper', type='Paper', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Psychology', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Machine Learning', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Robotics', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Developmental Synergy', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Feed-Forward Deep Learning Model', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Human Social Cognitive Development', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Adaptive Social Robots', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 12:\n",
      "  Nodes: [Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), Node(id='Artificial Intelligence (Ai)', type='Topic', properties={}), Node(id='This Paper', type='Paper', properties={}), Node(id='Model', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Artificial Intelligence (Ai)', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Model', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 13:\n",
      "  Nodes: [Node(id='Large Language Models', type='Topic', properties={}), Node(id=\"Guilford'S Structure Of Intellect (Soi) Model\", type='Method', properties={}), Node(id='Cognitive Prompt Engineering', type='Method', properties={}), Node(id='Novel Cognitive Prompting Approach', type='Method', properties={}), Node(id='Position Paper', type='Paper', properties={'summary': 'This position paper presents a novel cognitive prompting approach for enforcing SOI-inspired reasoning for improving clarity, coherence, and adaptability in model responses.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Large Language Models', type='Topic', properties={}), target=Node(id=\"Guilford'S Structure Of Intellect (Soi) Model\", type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Large Language Models', type='Topic', properties={}), target=Node(id='Cognitive Prompt Engineering', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id=\"Guilford'S Structure Of Intellect (Soi) Model\", type='Method', properties={}), target=Node(id='Cognitive Prompt Engineering', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Position Paper', type='Paper', properties={}), target=Node(id='Novel Cognitive Prompting Approach', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Novel Cognitive Prompting Approach', type='Method', properties={}), target=Node(id=\"Guilford'S Structure Of Intellect (Soi) Model\", type='Method', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 14:\n",
      "  Nodes: [Node(id='Large Language Models', type='Topic', properties={}), Node(id='Theory-Of-Mind Tasks', type='Topic', properties={}), Node(id='Gpt-4', type='Method', properties={}), Node(id='Gpt-3.5 Variants', type='Method', properties={}), Node(id='Davinci-2', type='Method', properties={}), Node(id='Davinci-3', type='Method', properties={}), Node(id='Gpt-3.5-Turbo', type='Method', properties={}), Node(id='Reinforcement Learning From Human Feedback', type='Method', properties={}), Node(id='In-Context Learning', type='Method', properties={}), Node(id='Two-Shot Chain Of Thought Reasoning', type='Method', properties={}), Node(id='Step-By-Step Thinking Instructions', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Large Language Models', type='Topic', properties={}), target=Node(id='Theory-Of-Mind Tasks', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Gpt-4', type='Method', properties={}), target=Node(id='Theory-Of-Mind Tasks', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Gpt-3.5 Variants', type='Method', properties={}), target=Node(id='Theory-Of-Mind Tasks', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Gpt-4', type='Method', properties={}), target=Node(id='In-Context Learning', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-3.5 Variants', type='Method', properties={}), target=Node(id='In-Context Learning', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-4', type='Method', properties={}), target=Node(id='Reinforcement Learning From Human Feedback', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Davinci-3', type='Method', properties={}), target=Node(id='Reinforcement Learning From Human Feedback', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-3.5-Turbo', type='Method', properties={}), target=Node(id='Reinforcement Learning From Human Feedback', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-4', type='Method', properties={}), target=Node(id='Two-Shot Chain Of Thought Reasoning', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-3.5 Variants', type='Method', properties={}), target=Node(id='Two-Shot Chain Of Thought Reasoning', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-4', type='Method', properties={}), target=Node(id='Step-By-Step Thinking Instructions', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Gpt-3.5 Variants', type='Method', properties={}), target=Node(id='Step-By-Step Thinking Instructions', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 15:\n",
      "  Nodes: [Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), Node(id='Trust-Aware Robot Policy', type='Method', properties={}), Node(id='Dynamic Trust-Aware Reward Function', type='Method', properties={}), Node(id='Human-Robot Trust', type='Topic', properties={}), Node(id='Tom-Based Robot Policy', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Trust-Aware Robot Policy', type='Method', properties={}), target=Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Trust-Aware Robot Policy', type='Method', properties={}), target=Node(id='Human-Robot Trust', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Dynamic Trust-Aware Reward Function', type='Method', properties={}), target=Node(id='Trust-Aware Robot Policy', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Tom-Based Robot Policy', type='Method', properties={}), target=Node(id='Human-Robot Trust', type='Topic', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 16:\n",
      "  Nodes: [Node(id='Mtomnet', type='Method', properties={}), Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Belief Prediction Dataset', type='Dataset', properties={}), Node(id='Belief Dynamics Prediction Dataset', type='Dataset', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Mtomnet', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Mtomnet', type='Method', properties={}), target=Node(id='Belief Prediction Dataset', type='Dataset', properties={}), type='USES_DATASET', properties={}), Relationship(source=Node(id='Mtomnet', type='Method', properties={}), target=Node(id='Belief Dynamics Prediction Dataset', type='Dataset', properties={}), type='USES_DATASET', properties={})]\n",
      "---\n",
      "Document 17:\n",
      "  Nodes: [Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Large-Scale Neural Language Models', type='Topic', properties={}), Node(id='Symbolictom', type='Method', properties={}), Node(id='Tomi Benchmark', type='Dataset', properties={}), Node(id='Le Et Al., 2019', type='Author', properties={}), Node(id='Off-The-Shelf Neural Networks', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Symbolictom', type='Method', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Large-Scale Neural Language Models', type='Topic', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Symbolictom', type='Method', properties={}), target=Node(id='Off-The-Shelf Neural Networks', type='Topic', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Symbolictom', type='Method', properties={}), target=Node(id='Tomi Benchmark', type='Dataset', properties={}), type='USES_DATASET', properties={}), Relationship(source=Node(id='Le Et Al., 2019', type='Author', properties={}), target=Node(id='Tomi Benchmark', type='Dataset', properties={}), type='AUTHORED', properties={})]\n",
      "---\n",
      "Document 18:\n",
      "  Nodes: [Node(id='In The Present Paper, We Study A Simple Model Of Causal Cognition, Viewed As A Quest For Causal Models', type='Paper', properties={'summary': 'The paper studies a simple model of causal cognition, showing that under mild assumptions, self-confirming causal models exist, which perpetrate self-deception and challenge objectivity.'}), Node(id='Causal Cognition', type='Topic', properties={}), Node(id='Self-Confirming Causal Models', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='In The Present Paper, We Study A Simple Model Of Causal Cognition, Viewed As A Quest For Causal Models', type='Paper', properties={}), target=Node(id='Causal Cognition', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='In The Present Paper, We Study A Simple Model Of Causal Cognition, Viewed As A Quest For Causal Models', type='Paper', properties={}), target=Node(id='Self-Confirming Causal Models', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 19:\n",
      "  Nodes: [Node(id='Bdiqa', type='Dataset', properties={'title': 'BDIQA: Benchmark for Cognitive Reasoning in VideoQA', 'summary': \"BDIQA is the first benchmark to explore cognitive reasoning capabilities of VideoQA models in the context of Theory of Mind (ToM), inspired by children's cognitive development, offering tasks at two difficulty levels assessing Belief, Desire, and Intention (BDI) reasoning.\"}), Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Videoqa', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Bdiqa', type='Dataset', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Bdiqa', type='Dataset', properties={}), target=Node(id='Videoqa', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 20:\n",
      "  Nodes: [Node(id='Large Language Models', type='Topic', properties={}), Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Dorsal Medial Prefrontal Cortex Neurons', type='Topic', properties={}), Node(id='Hidden Embeddings', type='Topic', properties={}), Node(id='Tom Tasks', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Large Language Models', type='Topic', properties={}), target=Node(id='Theory Of Mind', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Large Language Models', type='Topic', properties={}), target=Node(id='Dorsal Medial Prefrontal Cortex Neurons', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Large Language Models', type='Topic', properties={}), target=Node(id='Hidden Embeddings', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Hidden Embeddings', type='Topic', properties={}), target=Node(id='Tom Tasks', type='Method', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 21:\n",
      "  Nodes: [Node(id='Integration Of Ai Subdisciplines', type='Paper', properties={}), Node(id='Large Language Models', type='Topic', properties={}), Node(id='Cognitive Architectures', type='Topic', properties={}), Node(id='Modular Approach', type='Method', properties={}), Node(id='Agency Approach', type='Method', properties={}), Node(id='Neuro-Symbolic Approach', type='Method', properties={}), Node(id='Chain-Of-Thought Prompting', type='Method', properties={}), Node(id='Augmented Llms', type='Method', properties={}), Node(id='Common Model Of Cognition', type='Method', properties={}), Node(id='Simulation Theory Of Cognition', type='Method', properties={}), Node(id='Society Of Mind Theory', type='Method', properties={}), Node(id='Lida Cognitive Architecture', type='Method', properties={}), Node(id='Clarion Cognitive Architecture', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Integration Of Ai Subdisciplines', type='Paper', properties={}), target=Node(id='Large Language Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Integration Of Ai Subdisciplines', type='Paper', properties={}), target=Node(id='Cognitive Architectures', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Integration Of Ai Subdisciplines', type='Paper', properties={}), target=Node(id='Modular Approach', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Integration Of Ai Subdisciplines', type='Paper', properties={}), target=Node(id='Agency Approach', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Integration Of Ai Subdisciplines', type='Paper', properties={}), target=Node(id='Neuro-Symbolic Approach', type='Method', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Modular Approach', type='Method', properties={}), target=Node(id='Chain-Of-Thought Prompting', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Modular Approach', type='Method', properties={}), target=Node(id='Augmented Llms', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Modular Approach', type='Method', properties={}), target=Node(id='Common Model Of Cognition', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Modular Approach', type='Method', properties={}), target=Node(id='Simulation Theory Of Cognition', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Agency Approach', type='Method', properties={}), target=Node(id='Society Of Mind Theory', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Agency Approach', type='Method', properties={}), target=Node(id='Lida Cognitive Architecture', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Neuro-Symbolic Approach', type='Method', properties={}), target=Node(id='Clarion Cognitive Architecture', type='Method', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 22:\n",
      "  Nodes: [Node(id='Humans With An Average Level Of Social Cognition', type='Topic', properties={}), Node(id='Nonverbal Communication Signals', type='Topic', properties={}), Node(id='Social Cognitive Ability', type='Topic', properties={}), Node(id='Human-Robot Interaction And Collaboration', type='Topic', properties={}), Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), Node(id='Object-Context Relations', type='Topic', properties={}), Node(id='Multimodal Video Dataset', type='Dataset', properties={}), Node(id='Artificial Intelligence (Ai) Systems', type='Topic', properties={}), Node(id='Deep Learning Models', type='Method', properties={}), Node(id='This Paper', type='Paper', properties={'summary': 'This paper uses the combined knowledge of Theory of Mind (ToM) and Object-Context Relations to investigate methods for enhancing collaboration between humans and autonomous systems in environments where verbal communication is prohibited. It proposes a novel and challenging multimodal video dataset for assessing the capability of AI systems in predicting human belief states in an object-context scenario.'})]\n",
      "  Relationships: [Relationship(source=Node(id='Humans With An Average Level Of Social Cognition', type='Topic', properties={}), target=Node(id='Nonverbal Communication Signals', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Social Cognitive Ability', type='Topic', properties={}), target=Node(id='Human-Robot Interaction And Collaboration', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Object-Context Relations', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Multimodal Video Dataset', type='Dataset', properties={}), type='USES_DATASET', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Artificial Intelligence (Ai) Systems', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Deep Learning Models', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 23:\n",
      "  Nodes: [Node(id='Theory Of Mind', type='Topic', properties={}), Node(id='Hi-Tom', type='Dataset', properties={}), Node(id='Large Language Models', type='Method', properties={}), Node(id='Higher-Order Theory Of Mind', type='Topic', properties={}), Node(id='Higher Order Theory Of Mind Benchmark', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Hi-Tom', type='Dataset', properties={}), target=Node(id='Higher Order Theory Of Mind Benchmark', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Hi-Tom', type='Dataset', properties={}), target=Node(id='Higher-Order Theory Of Mind', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Large Language Models', type='Method', properties={}), target=Node(id='Higher-Order Theory Of Mind', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Large Language Models', type='Method', properties={}), target=Node(id='Hi-Tom', type='Dataset', properties={}), type='USES_DATASET', properties={})]\n",
      "---\n",
      "Document 24:\n",
      "  Nodes: [Node(id='Social Reasoning', type='Topic', properties={}), Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), Node(id='Machine Learning Approaches To Tom', type='Method', properties={}), Node(id='Tommy', type='Method', properties={}), Node(id='New Suite Of Experiments', type='Dataset', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Social Reasoning', type='Topic', properties={}), target=Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Machine Learning Approaches To Tom', type='Method', properties={}), target=Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Tommy', type='Method', properties={}), target=Node(id='Theory Of Mind (Tom)', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='New Suite Of Experiments', type='Dataset', properties={}), target=Node(id='Tommy', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 25:\n",
      "  Nodes: [Node(id='Virtual World Cognitive Science', type='Method', properties={}), Node(id='Mental And Linguistic Representation', type='Topic', properties={}), Node(id='Cognitive Science', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Virtual World Cognitive Science', type='Method', properties={}), target=Node(id='Mental And Linguistic Representation', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Virtual World Cognitive Science', type='Method', properties={}), target=Node(id='Cognitive Science', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n",
      "Document 26:\n",
      "  Nodes: [Node(id='Prompt Engineering', type='Topic', properties={}), Node(id='Natural Language Processing', type='Topic', properties={}), Node(id='Vision-Language Modeling', type='Topic', properties={}), Node(id='Multimodal-To-Text Generation Models', type='Topic', properties={}), Node(id='Image-Text Matching Models', type='Topic', properties={}), Node(id='Text-To-Image Generation Models', type='Topic', properties={}), Node(id='Flamingo', type='Method', properties={}), Node(id='Clip', type='Method', properties={}), Node(id='Stable Diffusion', type='Method', properties={}), Node(id='This Paper', type='Paper', properties={'title': 'A Comprehensive Survey of Prompt Engineering on Vision-Language Models'})]\n",
      "  Relationships: [Relationship(source=Node(id='Prompt Engineering', type='Topic', properties={}), target=Node(id='Natural Language Processing', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Prompt Engineering', type='Topic', properties={}), target=Node(id='Vision-Language Modeling', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Prompt Engineering', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Multimodal-To-Text Generation Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Image-Text Matching Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='This Paper', type='Paper', properties={}), target=Node(id='Text-To-Image Generation Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Multimodal-To-Text Generation Models', type='Topic', properties={}), target=Node(id='Flamingo', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Image-Text Matching Models', type='Topic', properties={}), target=Node(id='Clip', type='Method', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Text-To-Image Generation Models', type='Topic', properties={}), target=Node(id='Stable Diffusion', type='Method', properties={}), type='RELATED_TO', properties={})]\n",
      "---\n",
      "Document 27:\n",
      "  Nodes: [Node(id='Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', type='Paper', properties={'title': 'Designing effective prompts is essential to guiding large language models (LLMs) toward desired responses'}), Node(id='Automated Prompt Engineering', type='Topic', properties={}), Node(id='Optimal Learning Framework', type='Method', properties={}), Node(id='Feature-Based Method', type='Method', properties={}), Node(id='Bayesian Regression', type='Method', properties={}), Node(id='Knowledge-Gradient (Kg) Policy', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', type='Paper', properties={}), target=Node(id='Automated Prompt Engineering', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', type='Paper', properties={}), target=Node(id='Optimal Learning Framework', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', type='Paper', properties={}), target=Node(id='Feature-Based Method', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', type='Paper', properties={}), target=Node(id='Bayesian Regression', type='Method', properties={}), type='USES_METHOD', properties={}), Relationship(source=Node(id='Designing Effective Prompts Is Essential To Guiding Large Language Models (Llms) Toward Desired Responses', type='Paper', properties={}), target=Node(id='Knowledge-Gradient (Kg) Policy', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 28:\n",
      "  Nodes: [Node(id='Whole Hog Thesis', type='Topic', properties={}), Node(id='Sophisticated Large Language Models', type='Topic', properties={}), Node(id='Chatgpt', type='Topic', properties={}), Node(id='Holistic Network Assumptions', type='Method', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Whole Hog Thesis', type='Topic', properties={}), target=Node(id='Sophisticated Large Language Models', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Whole Hog Thesis', type='Topic', properties={}), target=Node(id='Chatgpt', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Whole Hog Thesis', type='Topic', properties={}), target=Node(id='Holistic Network Assumptions', type='Method', properties={}), type='USES_METHOD', properties={})]\n",
      "---\n",
      "Document 29:\n",
      "  Nodes: [Node(id='Interaction With Large Language Models', type='Paper', properties={'title': 'Interaction with Large Language Models'}), Node(id='Prompting', type='Topic', properties={}), Node(id='Prompt Design', type='Topic', properties={}), Node(id='Prompt Engineering', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Interaction With Large Language Models', type='Paper', properties={}), target=Node(id='Prompting', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Interaction With Large Language Models', type='Paper', properties={}), target=Node(id='Prompt Design', type='Topic', properties={}), type='DISCUSSES', properties={}), Relationship(source=Node(id='Interaction With Large Language Models', type='Paper', properties={}), target=Node(id='Prompt Engineering', type='Topic', properties={}), type='DISCUSSES', properties={})]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# After converting to graph documents\n",
    "for i, doc in enumerate(graph_documents):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  Nodes: {doc.nodes}\")\n",
    "    print(f\"  Relationships: {doc.relationships}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j\n",
      "system\n",
      "t20documentsgraph\n",
      "t30documentsgraph\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(uri=NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "with driver.session(database=\"system\") as session:\n",
    "    result = [record[\"name\"] for record in session.run(\"SHOW DATABASES\")]\n",
    "    for record in result:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neo4j', 'system', 't20documentsgraph', 't30documentsgraph']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\">  Test Vector RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign a Grader/Score to the Retriever documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Pipelin for the Retriever Grader\n",
    "\n",
    "# Ollama\n",
    "local_llm = \"llama3\"\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# OpenAI\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     temperature=1\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Theory of Mind (ToM) is the ability to reason about one's own and others'\n",
      "mental states. ToM plays a critical role in the development of intelligence,\n",
      "language understanding, and cognitive processes. While previous work has\n",
      "primarily focused on first and second-order ToM, we explore higher-order ToM,\n",
      "which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a\n",
      "Higher Order Theory of Mind benchmark. Our experimental evaluation using\n",
      "various Large Language Models (LLMs) indicates a decline in performance on\n",
      "higher-order ToM tasks, demonstrating the limitations of current LLMs. We\n",
      "conduct a thorough analysis of different failure cases of LLMs, and share our\n",
      "thoughts on the implications of our findings on the future of NLP.' metadata={'pk': 458163073511587863, 'summary': \"Theory of Mind (ToM) is the ability to reason about one's own and others'\\nmental states. ToM plays a critical role in the development of intelligence,\\nlanguage understanding, and cognitive processes. While previous work has\\nprimarily focused on first and second-order ToM, we explore higher-order ToM,\\nwhich involves recursive reasoning on others' beliefs. We introduce HI-TOM, a\\nHigher Order Theory of Mind benchmark. Our experimental evaluation using\\nvarious Large Language Models (LLMs) indicates a decline in performance on\\nhigher-order ToM tasks, demonstrating the limitations of current LLMs. We\\nconduct a thorough analysis of different failure cases of LLMs, and share our\\nthoughts on the implications of our findings on the future of NLP.\", 'title': 'HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models', 'url': 'http://arxiv.org/abs/2310.16755v1'}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Documents based on quesiton\n",
    "question = \"Do we have articles that talk about theory of mind?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "single_doc = docs[1]\n",
    "print(single_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is our answer relevant to the question asked: {'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "# Test Retrieval grade pipeline\n",
    "print(\n",
    "    f'Is our answer relevant to the question asked: {retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Asnwer based on retrived documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we have articles that discuss the theory of mind, covering topics such as the development of intelligence, cognitive processes, and the limitations of current AI systems in understanding mental states. These articles introduce models and benchmarks to test and improve Theory of Mind reasoning in artificial intelligence. Additionally, they explore practical applications and potential enhancements for social intelligence through cognitive knowledge graphs and generation models tailored for cognitive reasoning.\n"
     ]
    }
   ],
   "source": [
    "# Test Vector RAG chain\n",
    "question = \"Do we have articles that talk about theory of mind?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"yellow\"> Generate cypher to retrieve information from Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection ResolvedIPv4Address(('35.241.237.34', 7687)) (ResolvedIPv4Address(('35.241.237.34', 7687)))\n",
      "Failed to write data to connection IPv4Address(('a154a864.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('35.241.237.34', 7687)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Paper)\n",
      "WHERE toLower(p.title) CONTAINS toLower(\"Multi-Agent\")\n",
      "RETURN p.title, p.id\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'What paper talks about Multi-Agent?', 'result': [{'p.title': 'Multi-Agent Assistant Code Generation (AgentCoder)', 'p.id': 'Multi-Agent Assistant Code Generation (Agentcoder)'}, {'p.title': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)', 'p.id': 'This Article'}, {'p.title': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework', 'p.id': 'This Work'}], 'intermediate_steps': [{'query': 'cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\"Multi-Agent\")\\nRETURN p.title, p.id\\n'}]}\n"
     ]
    }
   ],
   "source": [
    "cypher_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert at generating Cypher queries for Neo4j.\n",
    "    Use the following schema to generate a Cypher query that answers the given question.\n",
    "    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\n",
    "    Focus on searching paper titles as they contain the most relevant information.\n",
    "    \n",
    "    Schema:\n",
    "    {schema}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Cypher Query:\"\"\",\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following Cypher query results to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise. If topic information is not available, focus on the paper titles.\n",
    "    \n",
    "    Question: {question} \n",
    "    Cypher Query: {query}\n",
    "    Query Results: {context} \n",
    "    \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"question\", \"query\", \"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "graph_rag_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=llm,\n",
    "    qa_llm=llm,\n",
    "    validate_cypher=True,\n",
    "    graph=kg,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    return_direct=True,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt,\n",
    ")\n",
    "\n",
    "question = \"What paper talks about Multi-Agent?\"\n",
    "generation = graph_rag_chain.invoke({\"query\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Composite Vector + Graph Generations\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Vector Context: {context} \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\", \"graph_context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Example input data\n",
    "question = \"What paper talk about Multi-Agent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'pk': 453568971862704139, 'summary': 'Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs, allowing for a flexible yet\\nstructured approach to problem-solving. A comprehensive evaluation across six\\nreal-world scenarios demonstrates that Captain Agent significantly outperforms\\nexisting multi-agent methods with 21.94% improvement in average accuracy,\\nproviding outstanding performance without requiring task-specific prompt\\nengineering. Our exploration of different backbone LLM and cost analysis\\nfurther shows that Captain Agent can improve the conversation quality of weak\\nLLM and achieve competitive performance with extremely low cost, which\\nilluminates the application of multi-agent systems.', 'title': 'Adaptive In-conversation Team Building for Language Model Agents', 'url': 'http://arxiv.org/abs/2405.19425v2'}, page_content='Leveraging multiple large language model (LLM) agents has shown to be a\\npromising approach for tackling complex tasks, while the effective design of\\nmultiple agents for a particular application remains an art. It is thus\\nintriguing to answer a critical question: Given a task, how can we build a team\\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\\noffers a flexible solution, realized through a novel agent design named Captain\\nAgent. It dynamically forms and manages teams for each step of a task-solving\\nprocess, utilizing nested group conversations and reflection to ensure diverse\\nexpertise and prevent stereotypical outputs, allowing for a flexible yet\\nstructured approach to problem-solving. A comprehensive evaluation across six\\nreal-world scenarios demonstrates that Captain Agent significantly outperforms\\nexisting multi-agent methods with 21.94% improvement in average accuracy,\\nproviding outstanding performance without requiring task-specific prompt\\nengineering. Our exploration of different backbone LLM and cost analysis\\nfurther shows that Captain Agent can improve the conversation quality of weak\\nLLM and achieve competitive performance with extremely low cost, which\\nilluminates the application of multi-agent systems.'), Document(metadata={'pk': 453568971862704140, 'summary': \"This technical report presents the Drama Engine, a novel framework for\\nagentic interaction with large language models designed for narrative purposes.\\nThe framework adapts multi-agent system principles to create dynamic,\\ncontext-aware companions that can develop over time and interact with users and\\neach other. Key features include multi-agent workflows with delegation, dynamic\\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\\nelements such as companion development, mood systems, and automatic context\\nsummarising. It is implemented in TypeScript. The framework's applications\\ninclude multi-agent chats and virtual co-workers for creative writing. The\\npaper discusses the system's architecture, prompt assembly process, delegation\\nmechanisms, and moderation techniques, as well as potential ethical\\nconsiderations and future extensions.\", 'title': 'Drama Engine: A Framework for Narrative Agents', 'url': 'http://arxiv.org/abs/2408.11574v1'}, page_content=\"This technical report presents the Drama Engine, a novel framework for\\nagentic interaction with large language models designed for narrative purposes.\\nThe framework adapts multi-agent system principles to create dynamic,\\ncontext-aware companions that can develop over time and interact with users and\\neach other. Key features include multi-agent workflows with delegation, dynamic\\nprompt assembly, and model-agnostic design. The Drama Engine introduces unique\\nelements such as companion development, mood systems, and automatic context\\nsummarising. It is implemented in TypeScript. The framework's applications\\ninclude multi-agent chats and virtual co-workers for creative writing. The\\npaper discusses the system's architecture, prompt assembly process, delegation\\nmechanisms, and moderation techniques, as well as potential ethical\\nconsiderations and future extensions.\"), Document(metadata={'pk': 453568971862704130, 'summary': 'The final frontier for simulation is the accurate representation of complex,\\nreal-world social systems. While agent-based modeling (ABM) seeks to study the\\nbehavior and interactions of agents within a larger system, it is unable to\\nfaithfully capture the full complexity of human-driven behavior. Large language\\nmodels (LLMs), like ChatGPT, have emerged as a potential solution to this\\nbottleneck by enabling researchers to explore human-driven interactions in\\npreviously unimaginable ways. Our research investigates simulations of human\\ninteractions using LLMs. Through prompt engineering, inspired by Park et al.\\n(2023), we present two simulations of believable proxies of human behavior: a\\ntwo-agent negotiation and a six-agent murder mystery game.', 'title': 'Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering', 'url': 'http://arxiv.org/abs/2308.07411v1'}, page_content='The final frontier for simulation is the accurate representation of complex,\\nreal-world social systems. While agent-based modeling (ABM) seeks to study the\\nbehavior and interactions of agents within a larger system, it is unable to\\nfaithfully capture the full complexity of human-driven behavior. Large language\\nmodels (LLMs), like ChatGPT, have emerged as a potential solution to this\\nbottleneck by enabling researchers to explore human-driven interactions in\\npreviously unimaginable ways. Our research investigates simulations of human\\ninteractions using LLMs. Through prompt engineering, inspired by Park et al.\\n(2023), we present two simulations of believable proxies of human behavior: a\\ntwo-agent negotiation and a six-agent murder mystery game.'), Document(metadata={'pk': 453568971862704129, 'summary': 'Two ways has been discussed to unlock the reasoning capability of a large\\nlanguage model. The first one is prompt engineering and the second one is to\\ncombine the multiple inferences of large language models, or the multi-agent\\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\\nmechanisms from the symmetry of agents. Empirically, this paper reports the\\nempirical results of the interplay of prompts and discussion mechanisms,\\nrevealing the empirical state-of-the-art performance of complex multi-agent\\nmechanisms can be approached by carefully developed prompt engineering. This\\npaper also proposes a scalable discussion mechanism based on conquer and merge,\\nproviding a simple multi-agent discussion solution with simple prompts but\\nstate-of-the-art performance.', 'title': 'On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts', 'url': 'http://arxiv.org/abs/2311.07076v1'}, page_content='Two ways has been discussed to unlock the reasoning capability of a large\\nlanguage model. The first one is prompt engineering and the second one is to\\ncombine the multiple inferences of large language models, or the multi-agent\\ndiscussion. Theoretically, this paper justifies the multi-agent discussion\\nmechanisms from the symmetry of agents. Empirically, this paper reports the\\nempirical results of the interplay of prompts and discussion mechanisms,\\nrevealing the empirical state-of-the-art performance of complex multi-agent\\nmechanisms can be approached by carefully developed prompt engineering. This\\npaper also proposes a scalable discussion mechanism based on conquer and merge,\\nproviding a simple multi-agent discussion solution with simple prompts but\\nstate-of-the-art performance.')]\n"
     ]
    }
   ],
   "source": [
    "# Get vector + graph answers\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The papers that talk about Multi-Agent are:\n",
      "\n",
      "* \"Adaptive In-conversation Team Building for Language Model Agents\"\n",
      "* \"Drama Engine: A Framework for Narrative Agents\"\n",
      "* \"Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering\"\n",
      "* \"On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts\"\n",
      "\n",
      "These papers discuss various aspects of multi-agent systems, including team building, narrative agents, simulations of human interactions, and discussion mechanisms.\n"
     ]
    }
   ],
   "source": [
    "vector_context = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "print(vector_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Paper)\n",
      "WHERE toLower(p.title) CONTAINS toLower(\"multi-agent\")\n",
      "RETURN p.title, p.id\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'What paper talk about Multi-Agent?', 'result': [{'p.title': 'Multi-Agent Assistant Code Generation (AgentCoder)', 'p.id': 'Multi-Agent Assistant Code Generation (Agentcoder)'}, {'p.title': 'Framework for Automatically Generating Process Models with Multi-Agent Orchestration (MAO)', 'p.id': 'This Article'}, {'p.title': 'Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting Framework', 'p.id': 'This Work'}], 'intermediate_steps': [{'query': 'cypher\\nMATCH (p:Paper)\\nWHERE toLower(p.title) CONTAINS toLower(\"multi-agent\")\\nRETURN p.title, p.id\\n'}]}\n"
     ]
    }
   ],
   "source": [
    "graph_context = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "print(graph_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The papers that talk about Multi-Agent are:\n",
      "\n",
      "* \"Adaptive In-conversation Team Building for Language Model Agents\"\n",
      "* \"Drama Engine: A Framework for Narrative Agents\"\n",
      "* \"Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering\"\n",
      "* \"On the Discussion of Large Language Models: Symmetry of Agents and Interplay with Prompts\"\n",
      "\n",
      "These papers discuss various aspects of multi-agent systems, including team building, narrative agents, simulations of human interactions, and discussion mechanisms.\n"
     ]
    }
   ],
   "source": [
    "composite_chain = prompt | llm | StrOutputParser()\n",
    "answer = composite_chain.invoke(\n",
    "    {\"question\": question, \"context\": vector_context, \"graph_context\": graph_context}\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    {documents} \n",
    "\n",
    "    Here is the answer: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     \n",
    "    Here is the answer:\n",
    "    {generation} \n",
    "\n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at routing a user question to the most appropriate data source. \n",
    "    You have three options:\n",
    "    1. 'vectorstore': Use for questions about LLM agents, prompt engineering, and adversarial attacks.\n",
    "    2. 'graphrag': Use for questions that involve relationships between entities, such as authors, papers, and topics, or when the question requires understanding connections between concepts.\n",
    "    3. 'web_search': Use for all other questions or when current information is needed.\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Choose the most appropriate option based on the nature of the question.\n",
    "\n",
    "    Return a JSON with a single key 'datasource' and no preamble or explanation. \n",
    "    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\n",
    "    \n",
    "    Question to route: \n",
    "    {question}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"llm agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement these as a control flow in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "        graph_context: results from graph search\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    graph_context: str\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents and graph context\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    graph_context = state.get(\"graph_context\", \"\")\n",
    "\n",
    "    # Composite RAG generation\n",
    "    generation = composite_chain.invoke(\n",
    "        {\"question\": question, \"context\": documents, \"graph_context\": graph_context}\n",
    "    )\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"graph_context\": graph_context,\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])  # Use get() with a default empty list\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "\n",
    "    if source[\"datasource\"] == \"graphrag\":\n",
    "        print(\"---TRYING GRAPH SEARCH---\")\n",
    "        graph_result = graph_search({\"question\": question})\n",
    "        if graph_result[\"graph_context\"] != \"No results found in the graph database.\":\n",
    "            return \"graphrag\"\n",
    "        else:\n",
    "            print(\"---NO RESULTS IN GRAPH, FALLING BACK TO VECTORSTORE---\")\n",
    "            return \"retrieve\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO VECTORSTORE RAG---\")\n",
    "        return \"retrieve\"\n",
    "    elif source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def graph_search(state):\n",
    "    \"\"\"\n",
    "    Perform GraphRAG search using Neo4j\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with graph search results\n",
    "    \"\"\"\n",
    "    print(\"---GRAPH SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use the graph_rag_chain to perform the search\n",
    "    result = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "    # Extract the relevant information from the result\n",
    "    # Adjust this based on what graph_rag_chain returns\n",
    "    graph_context = result.get(\"result\", \"\")\n",
    "\n",
    "    # You might want to combine this with existing documents or keep it separate\n",
    "    return {\"graph_context\": graph_context, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = grade = score.get(\"score\", \"\").lower()\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"graphrag\", graph_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set conditional entry point\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"graphrag\": \"graphrag\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"graphrag\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "What are the types of Prompt Engineering?\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO VECTORSTORE RAG---\n",
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n",
      "'Finished running: generate:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the types of Prompt Engineering?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinished running: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/milvus-bootcamp-rag-MiiP0ihC-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1221\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1216\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1217\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1218\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1219\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1220\u001b[0m ):\n\u001b[0;32m-> 1221\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/milvus-bootcamp-rag-MiiP0ihC-py3.11/lib/python3.11/site-packages/langgraph/pregel/runner.py:58\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, timeout, retry_policy)\u001b[0m\n\u001b[1;32m     56\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m futures:\n\u001b[0;32m---> 58\u001b[0m     done, _ \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/concurrent/futures/_base.py:305\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    303\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 305\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"What are the types of Prompt Engineering?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "Did Emmanuel Macron visit Germany recently?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Yes, Emmanuel Macron visited Germany recently. He arrived on May 26 for the '\n",
      " 'first state visit by a French president in 24 years. The trip was meant to '\n",
      " 'ease recent tensions and show unity between France and Germany.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"Did Emmanuel Macron visit Germany recently?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "Which paper talk about Collaborative Multi-Agent?\n",
      "{'datasource': 'graphrag'}\n",
      "graphrag\n",
      "---TRYING GRAPH SEARCH---\n",
      "---GRAPH SEARCH---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Paper)\n",
      "WHERE toLower(p.title) CONTAINS toLower(\"Collaborative Multi-Agent\")\n",
      "RETURN p.title, p.id\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "---GRAPH SEARCH---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Paper)\n",
      "WHERE toLower(p.title) CONTAINS toLower(\"Collaborative Multi-Agent\")\n",
      "RETURN p.title, p.id\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "'Finished running: graphrag:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('The paper \"Collaborative Multi-Agent, Multi-Reasoning-Path (CoMM) Prompting '\n",
      " 'Framework\" discusses Collaborative Multi-Agent.')\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"Which paper talk about Collaborative Multi-Agent?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
